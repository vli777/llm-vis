# Jupyter Notebook Export — Implementation Plan

## Goal

After an EDA run completes, users can click "Export as Notebook" and download a self-contained `.ipynb` file that reproduces every analysis step with pure pandas/matplotlib/seaborn code. The notebook is ready to open in JupyterLab, VS Code, or Colab — no dependency on the Auto-EDA backend.

---

## Architecture Decision

**Approach: Server-side .ipynb generation from `EDAReport`**

The `.ipynb` format is just JSON (`nbformat` v4). We generate it on the backend from the existing `EDAReport` + `DataProfile` + `ViewResult` objects. No Jupyter runtime needed — just JSON assembly.

Why not client-side? The backend has all the data (DataFrame, profile, view plans) and the domain knowledge of how each builder works. The frontend just triggers a download.

---

## Scope

### Phase 1 — Core charts (MVP)
Generate notebook code for the 9 standard chart types: bar, line, scatter, hist, box, table, heatmap, pie, area.

### Phase 2 — Advanced builders
Add code generation for the 30+ specialized builders (regression, SHAP, LGBM, etc.). These are already Python — the templates are close to what `build_view.py` does internally.

### Phase 3 — Polish
Parameterized setup cell (CSV path vs inline data), Colab badge, table of contents.

This plan covers **Phase 1** only.

---

## Files to Create/Modify

### New files
| File | Purpose |
|------|---------|
| `backend/skills/export_notebook.py` | Core module: `EDAReport` → `.ipynb` JSON |

### Modified files
| File | Change |
|------|--------|
| `backend/server/api.py` | Add `GET /api/runs/{run_id}/notebook` endpoint |
| `frontend/app/page.tsx` | Add "Export as Notebook" button per report |
| `frontend/lib/api.ts` | Add `downloadNotebook(runId)` helper (or just `window.open`) |

---

## Detailed Design

### 1. `backend/skills/export_notebook.py`

#### Public API

```python
def report_to_notebook(report: EDAReport, df: pd.DataFrame) -> dict:
    """Convert an EDAReport into nbformat-v4 notebook JSON (dict)."""
```

Returns a Python dict that is valid `.ipynb` JSON. No `nbformat` library dependency — we build the structure manually since it's a simple schema.

#### Notebook cell structure

```
Cell 0  [markdown]  # EDA Report: {table_name}
                     Generated by Auto-EDA on {date}. {row_count} rows, {col_count} columns.

Cell 1  [code]      # Setup: imports + load data
                     import pandas as pd
                     import matplotlib.pyplot as plt
                     import matplotlib
                     matplotlib.rcParams["figure.figsize"] = (10, 5)
                     import warnings; warnings.filterwarnings("ignore")

                     df = pd.read_csv("data.csv")  # <-- user updates path
                     df.shape

Cell 2  [markdown]  ## Summary Statistics

Cell 3  [code]      df.describe(include="all").T

For each StepResult in report.steps:
    Cell N   [markdown]  ## {step.headline}
                         {step.findings as bullet list}

    For each view_id in step.views:
        view = lookup view by id
        Cell N+1 [markdown]  ### {view.spec.title}
                              {view.explanation}
        Cell N+2 [code]      {_view_to_code(view)}
```

#### Code generation per chart type

Each function takes a `ViewResult` and returns a string of Python code.

**Bar chart** (`_code_bar`):
```python
x_field = view.plan.encoding.x.field
y_field = view.plan.encoding.y.field
agg = view.plan.encoding.y.aggregate or "count"
top_n = view.plan.options.top_n

if agg == "count":
    code = f'counts = df["{x_field}"].value_counts()'
    if top_n: code += f'.head({top_n})'
    code += f'\ncounts.plot.barh()\nplt.xlabel("Count")\nplt.ylabel("{x_field}")'
else:
    code = f'grouped = df.groupby("{x_field}")["{y_field}"].{agg}().sort_values(ascending=False)'
    if top_n: code += f'.head({top_n})'
    code += f'\ngrouped.plot.barh()\nplt.xlabel("{y_field} ({agg})")\nplt.ylabel("{x_field}")'

code += '\nplt.title("{title}")\nplt.tight_layout()\nplt.show()'
```

**Line chart** (`_code_line`):
```python
# If temporal x: parse dates, resample, plot
# If color grouping: pivot and plot multiple series
# Else: simple sort + plot
```

**Scatter** (`_code_scatter`):
```python
f'df.plot.scatter(x="{x}", y="{y}", alpha=0.5)\nplt.title("{title}")\nplt.show()'
# + optional color, log scale, sample
```

**Histogram** (`_code_hist`):
```python
f'df["{field}"].dropna().hist(bins={bin_count})\nplt.title("{title}")\nplt.xlabel("{field}")\nplt.show()'
```

**Box plot** (`_code_box`):
```python
# If grouped: df.boxplot(column=y, by=x)
# Else: df[y].plot.box()
```

**Table** (`_code_table`):
```python
# If summary tag: df.describe(include="all").T
# Else: df[fields].head(100)
```

**Heatmap** (`_code_heatmap`):
```python
f'ct = pd.crosstab(df["{y}"], df["{x}"])\nimport seaborn as sns\nsns.heatmap(ct, annot=True, fmt="d")\nplt.title("{title}")\nplt.show()'
```

**Pie** (`_code_pie`):
```python
f'df["{cat}"].value_counts().plot.pie(autopct="%1.1f%%")\nplt.title("{title}")\nplt.ylabel("")\nplt.show()'
# or with value column aggregation
```

**Area** (`_code_area`):
```python
# Same as line but with .plot.area()
```

#### Helper: notebook JSON structure

```python
def _make_notebook(cells: list[dict]) -> dict:
    return {
        "nbformat": 4,
        "nbformat_minor": 5,
        "metadata": {
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3",
            },
            "language_info": {"name": "python", "version": "3.10.0"},
        },
        "cells": cells,
    }

def _md_cell(source: str) -> dict:
    return {
        "cell_type": "markdown",
        "metadata": {},
        "source": source.splitlines(keepends=True),
    }

def _code_cell(source: str) -> dict:
    return {
        "cell_type": "code",
        "metadata": {},
        "source": source.splitlines(keepends=True),
        "outputs": [],
        "execution_count": None,
    }
```

#### Mapping dispatch

```python
_CODE_GENERATORS = {
    ChartType.bar: _code_bar,
    ChartType.line: _code_line,
    ChartType.scatter: _code_scatter,
    ChartType.hist: _code_hist,
    ChartType.box: _code_box,
    ChartType.table: _code_table,
    ChartType.heatmap: _code_heatmap,
    ChartType.pie: _code_pie,
    ChartType.area: _code_area,
}
```

For views with specialized tags (regression, SHAP, etc.) that don't have a code generator yet, emit a markdown cell: *"Advanced analysis ({tag}) — code generation not yet supported. See data below:"* followed by a code cell that displays the `data_inline` as a DataFrame literal.

#### Fallback for any view

If code generation fails or the chart type is unknown, embed the pre-computed data as a literal:

```python
data = {json.dumps(view.data_inline, indent=2)}
pd.DataFrame(data)
```

This ensures the notebook is always complete even if a code generator has a bug.

---

### 2. `backend/server/api.py` — New endpoint

```python
@router.get("/runs/{run_id}/notebook")
async def export_notebook(request: Request, run_id: str):
    """Download the EDA run as a Jupyter notebook."""
    sid = _require_session_id(request)
    report = get_run(run_id)
    if report is None:
        raise HTTPException(status_code=404, detail="Run not found.")

    # Get the DataFrame for this table
    sess = get_session(sid)
    if report.table_name not in sess:
        raise HTTPException(status_code=400, detail="Table data no longer available.")
    df = sess[report.table_name]

    nb = report_to_notebook(report, df)

    filename = f"eda_{report.table_name}.ipynb"
    return Response(
        content=json.dumps(nb, indent=1, default=str),
        media_type="application/x-ipynb+json",
        headers={"Content-Disposition": f'attachment; filename="{filename}"'},
    )
```

---

### 3. Frontend changes

#### `page.tsx`

Add an "Export Notebook" button next to each completed report. Minimal change — just a download link:

```tsx
<button
  onClick={() => {
    const sid = getSessionId();
    window.open(
      `${API_BASE}/api/runs/${report.run_id}/notebook`,
      "_blank"
    );
    // Note: need to pass session header — use fetch + blob instead
  }}
>
  Export as Notebook
</button>
```

Since the endpoint requires `X-Session-Id` header, we can't use a simple `window.open`. Instead:

```tsx
const downloadNotebook = async (runId: string) => {
  const res = await fetch(`${API_BASE}/api/runs/${runId}/notebook`, {
    headers: { "X-Session-Id": getSessionId() },
  });
  const blob = await res.blob();
  const url = URL.createObjectURL(blob);
  const a = document.createElement("a");
  a.href = url;
  a.download = `eda_report.ipynb`;
  a.click();
  URL.revokeObjectURL(url);
};
```

---

## Data Embedding Strategy

The generated notebook uses `pd.read_csv("data.csv")` by default. The first markdown cell instructs the user to update the path. We do **not** embed the full dataset inline (could be huge).

However, we include the column names and dtypes in a comment so users can verify they loaded the right file:

```python
# Expected columns: col_a (int64), col_b (object), col_c (float64), ...
# Expected rows: ~12345
df = pd.read_csv("data.csv")
print(f"Loaded {df.shape[0]} rows, {df.shape[1]} columns")
```

---

## What This Does NOT Do

- Does not run a Jupyter server or kernel
- Does not embed ipywidgets or interactive Recharts
- Does not convert the React frontend to Jupyter
- Does not require `nbformat` or `jupyter` as a Python dependency
- Does not support round-trip (editing notebook and importing back)

---

## Estimated Effort

| Task | Estimate |
|------|----------|
| `export_notebook.py` — notebook scaffold + helpers | ~100 lines |
| Code generators for 9 chart types | ~200 lines |
| Fallback/literal embedding | ~30 lines |
| API endpoint | ~20 lines |
| Frontend download button | ~30 lines |
| **Total new code** | **~380 lines** |
| Testing with real reports | ~2 hours |

---

## Implementation Order

1. `export_notebook.py` — cell helpers (`_md_cell`, `_code_cell`, `_make_notebook`)
2. `export_notebook.py` — code generators for each chart type
3. `export_notebook.py` — `report_to_notebook()` main function
4. `api.py` — `/runs/{run_id}/notebook` endpoint
5. `page.tsx` — download button + fetch helper
6. Manual test: upload CSV, run EDA, download .ipynb, open in Jupyter/VS Code
